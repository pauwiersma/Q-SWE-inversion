#!/bin/bash -l
#SBATCH --job-name=soil_compute
#SBATCH --mail-user pau.wiersma@unil.ch
#SBATCH --mail-type=NONE #you can write FAIL, ALL, NONE, 

#SBATCH --output=./slurm_output/%x_%A_%a.out
#SBATCH --error=./slurm_errors/%x_%A_%a.err
#SBATCH --time=00:10:00 #02:00:00
#SBATCH --mem=32G #limit is 512GB per node 
#SBATCH --partition=cpu
#SBATCH --nodes 1
#SBATCH --ntasks=1#  40
#SBATCH --cpus-per-task=1
#SBATCH --chdir /work/FAC/FGSE/IDYST/gmariet1/gaia/pwiersma/scripts/ewc

export EWC_RUNDIR=/work/FAC/FGSE/IDYST/gmariet1/gaia/pwiersma/scripts/ewc
export EWC_ROOTDIR=/work/FAC/FGSE/IDYST/gmariet1/gaia/pwiersma/ewatercycle

module purge #not really necessary if you have --export NONE
# dcsrsoft use 20241118
# module load openmpi

dcsrsoft use 20240303
module load mvapich2
module load python
module load micromamba

source ~/.bashrc
micromamba activate newcondatest

#For mpiruns: make sure the environmnet here is passed to the nodes 
export SLURM_EXPORT_ENV=ALL

BASIN=$1
RUN_ID=$2
TEST_FLAG=$3

# config_dir=/scratch/pwiersma/ewatercycle/experiments/config_files/${RUN_ID}_config.json
config_dir=/work/FAC/FGSE/IDYST/gmariet1/gaia/pwiersma/ewatercycle/experiments/config_files/${RUN_ID}_config.json

srun --cpu-bind=none python soil_compute.py $BASIN $config_dir  --test $TEST_FLAG

echo "Soilcalib of $RUN_ID completed"